# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# nsflow SDK Software in commercial settings.
#
# END COPYRIGHT
import logging
import os
import subprocess
import tempfile
from typing import Optional

from fastapi import APIRouter
from fastapi import File
from fastapi import Form
from fastapi import HTTPException
from fastapi import UploadFile
from fastapi.responses import JSONResponse

logging.basicConfig(level=logging.INFO)

# Adjust these to your repo/paths
REPO_DIR = os.getenv("FASTVLM_REPO_DIR")
if REPO_DIR is not None:
    PREDICT = os.path.join(REPO_DIR, "predict.py")
    DEFAULT_MODEL = os.path.join(REPO_DIR, "checkpoints", "fastvlm_0.5b_stage3")
    PYTHON_CMD = os.path.join(REPO_DIR, "venv/bin/python")

router = APIRouter(prefix="/api/v1")


@router.post("/vqa")
async def vqa(
    image: UploadFile = File(...),
    question: str = Form(...),
    model_path: Optional[str] = Form(None),
    timeout_sec: int = Form(120),
):
    """
    Answer a query about the image using the model

    Args:
        image: Image we have a query about
        question: Query about the image. Response generated by the model
        model_path: Path to the model to be used to answer the query
        timeout_sec: timeout in seconds


    Returns:
        JSON response containing the answer in text

    To test the endpoint with curl

    curl -X POST http://127.0.0.1:8005/api/v1/vqa \
        -F 'image=@/Users/joe/vacation.jpg' \
        -F 'question=How many people are in the photo?' \
        -F 'model_path=/Users/joe/Projects/ml-fastvlm-FORK/checkpoints/llava-fastvithd_7b_stage3'
    """
    model = model_path or DEFAULT_MODEL
    if REPO_DIR is None:
        raise HTTPException(501, "FASTVLM_REPO_DIR environment variable is not set! Set it to the full path to ml-fastvlm repo.")
    if not os.path.exists(PREDICT):
        raise HTTPException(500, f"predict.py not found at {PREDICT}")
    if not os.path.exists(model):
        raise HTTPException(500, f"model_path not found: {model}")

    # Save uploaded image to a temp file
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(image.filename or "")[-1]) as tmp:
            img_path = tmp.name
            content = await image.read()
            tmp.write(content)
    except Exception as e:
        raise HTTPException(400, f"Failed to save uploaded image: {e}")

    # Call predict.py
    cmd = [
        PYTHON_CMD,
        PREDICT,
        "--model-path",
        model,
        "--image-file",
        img_path,
        "--prompt",
        question,
    ]

    try:
        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout_sec,
            check=False,  # don’t raise automatically; we’ll return stderr if needed
        )
    except subprocess.TimeoutExpired:
        os.unlink(img_path)
        raise HTTPException(504, f"Prediction timed out after {timeout_sec}s")

    # Clean up the temp file
    try:
        os.unlink(img_path)
    except Exception:
        pass

    # Parse/return
    payload = {
        "exit_code": proc.returncode,
        "stdout": proc.stdout.strip(),
        "stderr": proc.stderr.strip(),
    }
    if proc.returncode != 0:
        # Bubble up a 500 with stderr for easier debugging
        raise HTTPException(500, detail=payload)

    # If predict.py prints extra text, you can post-process `proc.stdout` here.
    return JSONResponse(
        {
            "answer": proc.stdout.strip(),
            "model_path": model,
        }
    )
